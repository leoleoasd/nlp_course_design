
@misc{160301547Text,
  title = {[1603.01547] {{Text Understanding}} with the {{Attention Sum Reader Network}}},
  howpublished = {https://arxiv.org/abs/1603.01547},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\ND5ATSIQ\\1603.html}
}

@inproceedings{alemumogesMultiPerspectiveReasoningTransformers2021,
  title = {Multi-{{Perspective Reasoning Transformers}}},
  booktitle = {2021 13th {{International Conference}} on {{Machine Learning}} and {{Computing}}},
  author = {Alemu Moges, Dagmawi and Andre Niyongabo, Rubungo and Qu, Hong},
  year = {2021},
  pages = {503--508},
  doi = {10.1145/3457682.3457759},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\2IARYWVJ\\Alemu Moges 等。 - 2021 - Multi-Perspective Reasoning Transformers.pdf;C\:\\Users\\Leo\\Zotero\\storage\\QLMY837I\\3457682.html}
}

@article{baksiRecentAdvancesAutomated2021,
  title = {Recent {{Advances}} in {{Automated Question Answering In Biomedical Domain}}},
  author = {Baksi, Krishanu Das},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.05937},
  eprint = {2111.05937},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\KRY5BLTL\\Baksi - 2021 - Recent Advances in Automated Question Answering In.pdf;C\:\\Users\\Leo\\Zotero\\storage\\4Z6ZJI6S\\2111.html}
}

@article{baradaranSurveyMachineReading2020,
  title = {A Survey on Machine Reading Comprehension Systems},
  author = {Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
  year = {2020},
  journal = {arXiv preprint arXiv:2001.01582},
  eprint = {2001.01582},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\9MC6QWN7\\Baradaran 等。 - 2020 - A survey on machine reading comprehension systems.pdf;C\:\\Users\\Leo\\Zotero\\storage\\KGMEI2GG\\2001.html}
}

@inproceedings{beltagySciBERTPretrainedLanguage2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  pages = {3615--3620},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1371},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  annotation = {00896},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\ESI5V8L6\\Beltagy 等。 - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf}
}

@inproceedings{birdNLTKNaturalLanguage2004,
  title = {{{NLTK}}: {{The Natural Language Toolkit}}},
  shorttitle = {{{NLTK}}},
  booktitle = {Proceedings of the {{ACL Interactive Poster}} and {{Demonstration Sessions}}},
  author = {Bird, Steven and Loper, Edward},
  year = {2004},
  pages = {214--217},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}},
  doi = {10.3115/1219044.1219075},
  annotation = {02186},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\FUIRK54G\\Bird 和 Loper - 2004 - NLTK The Natural Language Toolkit.pdf}
}

@inproceedings{bordesTranslatingEmbeddingsModeling2013,
  title = {Translating {{Embeddings}} for {{Modeling Multi-relational Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\2KJQKC58\\Bordes 等。 - 2013 - Translating Embeddings for Modeling Multi-relation.pdf}
}

@inproceedings{chenMultichoiceRelationalReasoning2020,
  title = {Multi-Choice {{Relational Reasoning}} for {{Machine Reading Comprehension}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Chen, Wuya and Quan, Xiaojun and Kit, Chunyu and Min, Zhengcheng and Wang, Jiahai},
  year = {2020},
  pages = {6448--6458},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.567},
  abstract = {This paper presents our study of cloze-style reading comprehension by imitating human reading comprehension, which normally involves tactical comparing and reasoning over candidates while choosing the best answer. We propose a multi-choice relational reasoning (McR\$\^2\$) model with an aim to enable relational reasoning on candidates based on fusion representations of document, query and candidates. For the fusion representations, we develop an efficient encoding architecture by integrating the schemes of bidirectional attention flow, self-attention and document-gated query reading. Then, comparing and inferring over candidates are executed by a novel relational reasoning network. We conduct extensive experiments on four datasets derived from two public corpora, Children's Book Test and Who DiD What, to verify the validity and advantages of our model. The results show that it outperforms all baseline models significantly on the four benchmark datasets. The effectiveness of its key components is also validated by an ablation study.},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\YR997NDF\\Chen 等。 - 2020 - Multi-choice Relational Reasoning for Machine Read.pdf}
}

@article{cosmaApproachSourcecodePlagiarism2011,
  title = {An Approach to Source-Code Plagiarism Detection and Investigation Using Latent Semantic Analysis},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2011},
  journal = {IEEE transactions on computers},
  volume = {61},
  number = {3},
  pages = {379--394},
  publisher = {{IEEE}},
  doi = {10.1109/TC.2011.223},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\VFJTCQ3F\\Cosma 和 Joy - 2011 - An approach to source-code plagiarism detection an.pdf;C\:\\Users\\Leo\\Zotero\\storage\\INWNYAGF\\6086533.html}
}

@article{cosmaDefinitionSourcecodePlagiarism2008,
  title = {Towards a Definition of Source-Code Plagiarism},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2008},
  journal = {IEEE Transactions on Education},
  volume = {51},
  number = {2},
  pages = {195--200},
  publisher = {{IEEE}},
  doi = {10.1109/TE.2007.906776},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\F5QCP9SG\\Cosma 和 Joy - 2008 - Towards a definition of source-code plagiarism.pdf;C\:\\Users\\Leo\\Zotero\\storage\\GGMZGQGB\\4455461.html}
}

@misc{COVID19OpenResearch,
  title = {{{COVID-19 Open Research Dataset Challenge}} ({{CORD-19}})},
  abstract = {An AI challenge with AI2, CZI, MSR, Georgetown, NIH \& The White House},
  howpublished = {https://kaggle.com/allen-institute-for-ai/CORD-19-research-challenge},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\52F89UH5\\CORD-19-research-challenge.html}
}

@inproceedings{cuiAttentionoverAttentionNeuralNetworks2017,
  title = {Attention-over-{{Attention Neural Networks}} for {{Reading Comprehension}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
  year = {2017},
  pages = {593--602},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1055},
  abstract = {Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test.},
  annotation = {00396},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\MQZQE9FE\\Cui 等。 - 2017 - Attention-over-Attention Neural Networks for Readi.pdf;C\:\\Users\\Leo\\Zotero\\storage\\TISLTDP2\\Cui 等。 - 2016 - Attention-over-Attention Neural Networks for Readi.pdf}
}

@inproceedings{cuiConsensusAttentionbasedNeural2016,
  title = {Consensus {{Attention-based Neural Networks}} for {{Chinese Reading Comprehension}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
  year = {2016},
  pages = {1777--1786},
  publisher = {{The COLING 2016 Organizing Committee}},
  address = {{Osaka, Japan}},
  abstract = {Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.},
  keywords = {⛔ No DOI found},
  annotation = {00085},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\CN9QVBLW\\Cui 等。 - 2016 - Consensus Attention-based Neural Networks for Chin.pdf}
}

@article{dasigiDatasetInformationSeekingQuestions2021,
  title = {A {{Dataset}} of {{Information-Seeking Questions}} and {{Answers Anchored}} in {{Research Papers}}},
  author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.03011},
  eprint = {2105.03011},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\PWW3MQUA\\Dasigi 等。 - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf;C\:\\Users\\Leo\\Zotero\\storage\\PYL9KFZ5\\2105.html}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  annotation = {32749},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\6VRD9CD4\\Devlin 等。 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{dingOpenPromptOpensourceFramework2021,
  title = {{{OpenPrompt}}: {{An Open-source Framework}} for {{Prompt-learning}}},
  shorttitle = {{{OpenPrompt}}},
  author = {Ding, Ning and Hu, Shengding and Zhao, Weilin and Chen, Yulin and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01998 [cs]},
  eprint = {2111.01998},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to \$cloze\$-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt-learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc. need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present \{OpenPrompt\}, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at \{\textbackslash url\{ https://github.com/thunlp/OpenPrompt\}\}.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\2DIRFVHC\\Ding 等。 - 2021 - OpenPrompt An Open-source Framework for Prompt-le.pdf;C\:\\Users\\Leo\\Zotero\\storage\\DKD84SRA\\2111.html}
}

@misc{duanAttentionAllYou2020,
  title = {Attention {{Is All You Need}} for {{Chinese Word Segmentation}}},
  author = {Duan, Sufeng and Zhao, Hai},
  year = {2020},
  month = oct,
  number = {arXiv:1910.14537},
  eprint = {1910.14537},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1910.14537},
  abstract = {Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\5ZER8SPJ\\Duan 和 Zhao - 2020 - Attention Is All You Need for Chinese Word Segment.pdf;C\:\\Users\\Leo\\Zotero\\storage\\HCH7QV88\\1910.html}
}

@article{duBiomedicaldomainPretrainedLanguage2020,
  title = {Biomedical-Domain Pre-Trained Language Model for Extractive Summarization},
  author = {Du, Yongping and Li, Qingxiao and Wang, Lulin and He, Yanqing},
  year = {2020},
  month = jul,
  journal = {Knowledge-Based Systems},
  volume = {199},
  pages = {105964},
  issn = {09507051},
  doi = {10/gg286r},
  abstract = {In recent years, the performance of deep neural network in extractive summarization task has been improved significantly compared with traditional methods. However, in the field of biomedical extractive summarization, existing methods cannot make good use of the domain-aware external knowledge; furthermore, the document structural feature is omitted by existing deep neural network model. In this paper, we propose a novel model called BioBERTSum to better capture token-level and sentence-level contextual representation, which uses a domain-aware bidirectional language model pre-trained on large-scale biomedical corpora as encoder, and further fine-tunes the language model for extractive text summarization task on single biomedical document. Especially, we adopt a sentence position embedding mechanism, which enables the model to learn the position information of sentences and achieve the structural feature of document. To the best of our knowledge, this is the first work to use the pre-trained language model and fine-tuning strategy for extractive summarization task in the biomedical domain. Experiments on PubMed dataset show that our proposed model outperforms the recent SOTA (state-of-the-art) model by ROUGE-1/2/L.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\8XGDIJNP\\Du 等。 - 2020 - Biomedical-domain pre-trained language model for e.pdf}
}

@inproceedings{duDualModelWeighting2021,
  title = {Dual {{Model Weighting Strategy}} and {{Data Augmentation}} in {{Biomedical Question Answering}}},
  booktitle = {Proceedings of the 2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Yan, Jingya and Zhao, Yiliang and Lu, Yuxuan and Jin, Xingnan},
  year = {2021},
  abstract = {Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, it's more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings. We propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre- trained in Biomedical domain data. Specially, we adopt effective data augmentation strategies to improve the model performance, including slide window, summarization and round-trip trans- lation. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our approach. The results show that the model performance has been improved significantly on BioASQ 6B, 7B and 8B datasets compared to the single model.},
  annotation = {00000}
}

@inproceedings{duHierarchicalQuestionAwareContext2019,
  title = {Hierarchical {{Question-Aware Context Learning}} with {{Augmented Data}} for {{Biomedical Question Answering}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Guo, Wenyang and Zhao, Yiliang},
  year = {2019},
  month = nov,
  pages = {370--375},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10/gnm95v},
  abstract = {This paper is concerned with the task of biomedical Question Answering (QA) which refers to extracting an answer to the given question from a biomedical context. Current works have made progress on this task, but they are still severely restricted by the insufficient training data due to the domain-specific nature, which motivates us to further explore a powerful way to solve this problem. We propose a Hierarchical Question-Aware Context Learning (HQACL) model for the biomedical QA task constituted by multi-level attention. The interaction between the question and the context can be captured layer by layer, with multigrained embeddings to strengthen the ability of the language representation. A special training method called DA, including two parts namely domain adaptation and data augmentation, is also introduced to enhance the model performance. Domain adaptation can be defined as pre-training on a large-scale opendomain dataset and fine-tuning on the small training set of the target domain. As for the data augmentation, the Round-trip translation method is adopted to create new data with various expressions, which almost doubles the training set. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our model. The results show that our approach is superior to the best recent solution and achieves a new state of the art.},
  isbn = {978-1-72811-867-3},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\L8A96VKW\\Du 等。 - 2019 - Hierarchical Question-Aware Context Learning with .pdf}
}

@inproceedings{fuATNetAnsweringClozeStyle2019,
  title = {{{ATNet}}: {{Answering Cloze-Style Questions}} via {{Intra-attention}} and {{Inter-attention}}},
  shorttitle = {{{ATNet}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fu, Chengzhen and Li, Yuntao and Zhang, Yan},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {242--252},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-16145-3_19},
  abstract = {This paper proposes a novel framework, named ATNet, for answering cloze-style questions over documents. Our model, in the encoder phase, projects all contextual embeddings into multiple latent semantic spaces, with representations of each space attending to a specific aspect of semantics. Long-term dependencies among the whole document are captured via the intra-attention module. A gate is produced to control the degree to which the retrieved dependency information is fused and the previous token embedding is exposed. Then, in the interaction phase, the context is aligned with the query across different semantic spaces to achieve the information aggregation. Specifically, we compute inter-attention based on a sophisticated feature set. Experiments and ablation studies demonstrate the effectiveness of ATNet.},
  isbn = {978-3-030-16145-3},
  langid = {english},
  keywords = {Inter-attention,Intra-attention,Question answering},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\DCEV6ZCC\\Fu 等。 - 2019 - ATNet Answering Cloze-Style Questions via Intra-a.pdf}
}

@inproceedings{fuEAReaderEnhance2019,
  title = {{{EA Reader}}: {{Enhance Attentive Reader}} for {{Cloze-Style Question Answering}} via {{Multi-Space Context Fusion}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Fu, Chengzhen and Zhang, Yan},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {6375--6382},
  doi = {10.1609/aaai.v33i01.33016375},
  abstract = {\&lt;p\&gt;Query-document semantic interactions are essential for the success of many cloze-style question answering models. Recently, researchers have proposed several attention-based methods to predict the answer by focusing on appropriate subparts of the context document. In this paper, we design a novel module to produce the query-aware context vector, named Multi-Space based Context Fusion (MSCF), with the following considerations: (1) interactions are applied across multiple latent semantic spaces; (2) attention is measured at bit level, not at token level. Moreover, we extend MSCF to the multi-hop architecture. This unified model is called Enhanced Attentive Reader (EA Reader). During the iterative inference process, the reader is equipped with a novel memory update rule and maintains the understanding of documents through \&lt;em\&gt;read\&lt;/em\&gt;, \&lt;em\&gt;update\&lt;/em\&gt; and \&lt;em\&gt;write\&lt;/em\&gt; operations. We conduct extensive experiments on four real-world datasets. Our results demonstrate that EA Reader outperforms state-of-the-art models.\&lt;/p\&gt;},
  chapter = {AAAI Technical Track: Natural Language Processing}
}

@article{guDomainSpecificLanguageModel2021,
  title = {Domain-{{Specific Language Model Pretraining}} for {{Biomedical Natural Language Processing}}},
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {3},
  number = {1},
  pages = {2:1--2:23},
  issn = {2691-1957},
  doi = {10.1145/3458754},
  abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
  keywords = {Biomedical,domain-specific pretraining,NLP},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\BYIAH24B\\Gu 等。 - 2021 - Domain-Specific Language Model Pretraining for Bio.pdf}
}

@inproceedings{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  pages = {8342--8360},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.740},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\PTFYYZU8\\Gururangan 等。 - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf}
}

@article{guyingjieguixiaolinlidefushenyiliaodongJiYuShenJingWangLuoDeJiQiYueDuLiJieZongShu2020,
  title = {基于神经网络的机器阅读理解综述},
  author = {顾迎捷，桂小林，李德福，沈毅，廖东 and {Ying-Jie}, GU and {Xiao-Lin}, GUI and {De-Fu}, LI and Yi, Shen and Dong, Liao},
  year = {2020},
  month = apr,
  journal = {软件学报},
  volume = {31},
  number = {7},
  pages = {2095--2126},
  abstract = {机器阅读理解的目标是使机器理解自然语言文本，并能够正确回答与文本相关的问题.由于数据集规模的制约，早期的机器阅读理解方法大多基于人工特征以及传统机器学习方法进行建模.近年来，随着知识库、众包群智的发展，研究者们陆续提出了高质量的大规模数据集，为神经网络模型以及机器阅读理解的发展带来了新的契机.对基于神经网络的机器阅读理解相关的最新研究成果进行了详尽的归纳：首先，概述了机器阅读理解的发展历程、问题描述以及评价指标；然后，针对当前最流行的神经阅读理解模型架构，包括嵌入层、编码层、交互层和输出层中所使用的相关技术进行了全面的综述，同时阐述了最新的BERT预训练模型及其优势；之后，归纳了近年来机器阅读理解数据集和神经阅读理解模型的研究进展，同时，详细比较分析了最具代表性的数据集以及神经网络模型；最后展望了机器阅读理解研究所面临的挑战和未来的研究方向.;The task of machine reading comprehension is to make the machine understand natural language text and correctly answer text-related questions. Due to the limitation of the dataset scale, most of the early machine reading comprehension methods were modeled based on manual features and traditional machine learning methods. In recent years, with the development of knowledge bases and crowdsourcing, high quality large-scale datasets have been proposed by researchers, which has brought a new opportunity for the advance of neural network models and machine reading comprehension. In this survey, an exhaustive review on the state-of-the-art research efforts on machine reading comprehension based on neural network is made. First, an overview of machine reading comprehension, including development process, problem formulation, and evaluation metric, is given. Then, a comprehensive review is conducted of related technologies in the most fashionable neural reading comprehension framework including the embedding layer, encoder layer, interaction layer, and output layer as well as the latest BERT pre-training model and its advantages are discussed. After that, this paper concludes the recent research progress of machine reading comprehension datasets and neural reading comprehension model, and gives a comparison and analysis of the most representative datasets and neural network models in detail. Finally, the research challenges and future direction of machine reading comprehension are presented.},
  annotation = {00000},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\6DNKLGU6\\基于神经网络的机器阅读理解综述_顾迎捷.pdf;C\:\\Users\\Leo\\Zotero\\storage\\NI6ER732\\6048.html}
}

@inproceedings{hanOpenKEOpenToolkit2018,
  title = {{{OpenKE}}: {{An Open Toolkit}} for {{Knowledge Embedding}}},
  shorttitle = {{{OpenKE}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Han, Xu and Cao, Shulin and Lv, Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
  year = {2018},
  month = nov,
  pages = {139--144},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/d18-2024},
  abstract = {We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on http://openke.thunlp.org/.},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\WR3BZNEW\\Han 等。 - 2018 - OpenKE An Open Toolkit for Knowledge Embedding.pdf}
}

@inproceedings{hermannTeachingMachinesRead2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  annotation = {02578},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\54SJ7QAF\\Hermann 等。 - 2015 - Teaching Machines to Read and Comprehend.pdf}
}

@inproceedings{hillGoldilocksPrincipleReading2016,
  title = {The {{Goldilocks Principle}}: {{Reading Children}}'s {{Books}} with {{Explicit Memory Representations}}},
  shorttitle = {The {{Goldilocks Principle}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  keywords = {⛔ No DOI found},
  annotation = {00549}
}

@article{huangImprovedKnowledgeBase2016,
  title = {Improved {{Knowledge Base Completion}} by {{Path-Augmented TransR Model}}},
  author = {Huang, Wenhao and Li, Ge and Jin, Zhi},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.04073 [cs]},
  eprint = {1610.04073},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence},
  annotation = {00011},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\ANP2S6B6\\Huang 等。 - 2016 - Improved Knowledge Base Completion by Path-Augment.pdf;C\:\\Users\\Leo\\Zotero\\storage\\QFNPDAJA\\1610.html}
}

@inproceedings{huTaichiProgrammingLanguage2020,
  title = {The {{Taichi}} Programming Language},
  booktitle = {{{ACM SIGGRAPH}} 2020 {{Courses}}},
  author = {Hu, Yuanming},
  year = {2020},
  month = aug,
  series = {{{SIGGRAPH}} '20},
  pages = {1--50},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3388769.3407493},
  isbn = {978-1-4503-7972-4}
}

@article{ishidaWeNeedZero2021,
  title = {Do {{We Need Zero Training Loss After Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  year = {2021},
  month = mar,
  journal = {arXiv:2002.08709 [cs, stat]},
  eprint = {2002.08709},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero \textbackslash emph\{training error\}. Even after memorization, the \textbackslash emph\{training loss\} continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called \textbackslash emph\{flooding\} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the \textbackslash emph\{flood level\}. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to "random walk" with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\TMUX3S97\\Ishida 等。 - 2021 - Do We Need Zero Training Loss After Achieving Zero.pdf;C\:\\Users\\Leo\\Zotero\\storage\\UTXQTF8Z\\Ishida 等。 - 2021 - Do We Need Zero Training Loss After Achieving Zero.pdf;C\:\\Users\\Leo\\Zotero\\storage\\7KAJ2FX7\\2002.html}
}

@inproceedings{jiKnowledgeGraphEmbedding2015,
  title = {Knowledge {{Graph Embedding}} via {{Dynamic Mapping Matrix}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  year = {2015},
  month = jul,
  pages = {687--696},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-1067},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\Z6S2U4FY\\Ji 等。 - 2015 - Knowledge Graph Embedding via Dynamic Mapping Matr.pdf}
}

@article{jinBiomedicalQuestionAnswering2021,
  title = {Biomedical Question Answering: {{A}} Comprehensive Review},
  shorttitle = {Biomedical Question Answering},
  author = {Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.05281},
  eprint = {2102.05281},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\GHDZ82S6\\Jin 等。 - 2021 - Biomedical question answering A comprehensive rev.pdf;C\:\\Users\\Leo\\Zotero\\storage\\LBQLCUDE\\2102.html}
}

@article{jinWhatDiseaseDoes2020,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.13081 [cs]},
  eprint = {2009.13081},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7\textbackslash\%, 42.0\textbackslash\%, and 70.1\textbackslash\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\UEEAERKL\\Jin 等。 - 2020 - What Disease does this Patient Have A Large-scale.pdf}
}

@article{joySourceCodePlagiarism2010,
  title = {Source Code Plagiarism\textemdash a Student Perspective},
  author = {Joy, Mike and Cosma, Georgina and Yau, Jane Yin-Kim and Sinclair, Jane},
  year = {2010},
  journal = {IEEE Transactions on Education},
  volume = {54},
  number = {1},
  pages = {125--132},
  publisher = {{IEEE}},
  doi = {10.1109/TE.2010.2046664},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\C4PWKHWH\\Joy 等。 - 2010 - Source code plagiarism—a student perspective.pdf;C\:\\Users\\Leo\\Zotero\\storage\\6ZKS3JAQ\\5451097.html}
}

@inproceedings{kaddariBiomedicalQuestionAnswering2020,
  title = {Biomedical Question Answering: {{A}} Survey of Methods and Datasets},
  shorttitle = {Biomedical Question Answering},
  booktitle = {2020 {{Fourth International Conference On Intelligent Computing}} in {{Data Sciences}} ({{ICDS}})},
  author = {Kaddari, Zakaria and Mellah, Youssef and Berrich, Jamal and Bouchentouf, Toumi and Belkasmi, Mohammed G.},
  year = {2020},
  pages = {1--8},
  publisher = {{IEEE}},
  doi = {10.1109/icds50568.2020.9268742},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\Z6ZYD3BX\\Kaddari 等。 - 2020 - Biomedical question answering A survey of methods.pdf;C\:\\Users\\Leo\\Zotero\\storage\\MWA5EAS8\\9268742.html}
}

@inproceedings{kadlecTextUnderstandingAttention2016,
  title = {Text {{Understanding}} with the {{Attention Sum Reader Network}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kadlec, Rudolf and Schmid, Martin and Bajgar, Ondrej and Kleindienst, Jan},
  year = {2016},
  pages = {908--918},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-1086},
  annotation = {00295},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\PSDJ3PTZ\\Kadlec 等。 - 2016 - Text Understanding with the Attention Sum Reader N.pdf}
}

@inproceedings{kePretrainingMetaLearning2021,
  title = {Pre-Training with {{Meta Learning}} for {{Chinese Word Segmentation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ke, Zhen and Shi, Liang and Sun, Songtao and Meng, Erli and Wang, Bin and Qiu, Xipeng},
  year = {2021},
  pages = {5514--5523},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.436},
  abstract = {Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\B2YQSC44\\Ke 等。 - 2021 - Pre-training with Meta Learning for Chinese Word S.pdf}
}

@inproceedings{lanALBERTLiteBERT2019,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2019},
  month = sep,
  abstract = {A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\82F4KDJJ\\Lan 等。 - 2019 - ALBERT A Lite BERT for Self-supervised Learning o.pdf;C\:\\Users\\Leo\\Zotero\\storage\\UGQV4ID8\\forum.html}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  year = {2004},
  month = mar,
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  keywords = {Algorithm design and analysis,Application software,Arithmetic,High level languages,Information analysis,Performance analysis,Program processors,Runtime,Software safety,Virtual machining},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\JSTRY7FE\\1281665.html}
}

@inproceedings{leeAbstractModelingEvaluating2020,
  title = {Abstract: {{Modeling}} and {{Evaluating Intervention Options}} and {{Strategies}} for {{COVID-19 Containment}}: {{A Biological-Behavioral-Logistics Computation Decision Framework}}},
  shorttitle = {Abstract},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Lee, Eva},
  year = {2020},
  pages = {1--2},
  doi = {10.1109/BIBM49941.2020.9313325},
  abstract = {SARs, bird flu, H1N1, Ebola crisis in W. Africa, Zika and current SARS-CoV-2 underscore the critical importance of emergency response and medical preparedness. Such needs are wide-spread as globalization and air transportation facilitate rapid disease spread across the world. Computational modeling of infectious disease outbreaks and epidemics offer insights in propagation patterns and facilitate policy makers to synthesize potential interventions. Current models include inclined decay with an exponential adjustment, SEIR (susceptible, exposed, infectious, recovered) compartmental model, discrete time stochastic processes, and transmission tree. While many of these models incorporate contact tracing to predict spread pattern, key elements on optimal usage of scarce resources and effective and efficient process performance (e.g., diagnostics and screening, non-pharmaceutical interventions, trained personnel/robots for treatment, decontamination) have not been included. This is particularly critical in the fight of COVID-19 containment due to lack of testing kits and the prevalence of asymptomatic transmission, and the long period of hospitalization required by severely sick patients.This work focuses on designing a system computational decision modeling framework that simultaneously i) captures disease spread characteristics, ii) incorporates day-to-day hospital and home care processes and resource usage, iii) explores non-pharmaceutical intervention, social and human behavior and iv) allows for system optimization to minimize infection and mortality under time and labor constraints.},
  keywords = {Atmospheric modeling,Computational modeling,COVID-19,Logistics,Medical diagnostic imaging,Planning,Prediction algorithms},
  annotation = {00000},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\VHHQN388\\Lee - 2020 - Abstract Modeling and Evaluating Intervention Opt.pdf;C\:\\Users\\Leo\\Zotero\\storage\\PCWM423A\\9313325.html}
}

@article{leeBioBERTPretrainedBiomedical2019,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  editor = {Wren, Jonathan},
  year = {2019},
  month = sep,
  journal = {Bioinformatics},
  pages = {btz682},
  issn = {1367-4803, 1460-2059},
  doi = {10/ggh5qq},
  abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\KE8AYWD6\\Lee 等。 - 2019 - BioBERT a pre-trained biomedical language represe.pdf;C\:\\Users\\Leo\\Zotero\\storage\\YEY8CG7E\\Lee 等。 - 2019 - BioBERT a pre-trained biomedical language represe.pdf}
}

@inproceedings{linLearningEntityRelation2015,
  title = {Learning {{Entity}} and {{Relation Embeddings}} for {{Knowledge Graph Completion}}},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  year = {2015},
  month = feb,
  abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\8E9EYXE4\\Lin 等。 - 2015 - Learning Entity and Relation Embeddings for Knowle.pdf;C\:\\Users\\Leo\\Zotero\\storage\\XQKGVSBN\\9571.html}
}

@inproceedings{linModelingRelationPaths2015,
  title = {Modeling {{Relation Paths}} for {{Representation Learning}} of {{Knowledge Bases}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong and Rao, Siwei and Liu, Song},
  year = {2015},
  pages = {705--714},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1082},
  abstract = {Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation\_extraction.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\WLKY3R2P\\Lin 等。 - 2015 - Modeling Relation Paths for Representation Learnin.pdf}
}

@article{liuJiYuBiLSTMDeShuXueZhuGuanTiZiDongYueJuanFangFa2018,
  title = {{基于Bi-LSTM的数学主观题自动阅卷方法}},
  author = {刘, 逸雪 and 卢, 雨轩 and 丁, 亮 and 汪, 星明},
  year = {2018},
  journal = {管理观察},
  number = {02},
  pages = {109--113},
  issn = {1674-2877},
  abstract = {数学主观题自动阅卷既无法直接采用长文本计算中的TF-IDF等统计方法,又因为缺少相关知识库而无法使用语料库、知识库、语言学等短文本的方法。本文根据数学主观题的特点,提出了一种将人工制定评分标准和双向长短时记忆神经网络相结合的数学主观题自动阅卷方法,在高二年级数学真实考题上进行实验,准确率达到83.17\%。},
  langid = {chinese},
  keywords = {⛔ No DOI found,Automatic marking,Bi-LSTM,Deep learning,Mathematical subjective questions,Text similarity,数学主观题,文本相似度,深度学习,自动阅卷},
  annotation = {00000}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = sep,
  abstract = {We evaluate a number of design decisions when pretraining BERT models and propose an improved recipe that achieves state-of-the-art results on many natural language understanding tasks.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\LPEKI7VC\\Liu 等。 - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;C\:\\Users\\Leo\\Zotero\\storage\\JJCNPB3U\\forum.html}
}

@article{liuRoBERTaRobustlyOptimized2019a,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  annotation = {02721},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\CJTYAXKK\\Liu 等。 - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;C\:\\Users\\Leo\\Zotero\\storage\\BNHIDR7N\\1907.html}
}

@article{liuzhiyuansunmaosonglinyankaixieruobingZhiShiBiaoShiXueXiYanJiuJinZhan2016,
  title = {{知识表示学习研究进展}},
  author = {刘知远，孙茂松，林衍凯，谢若冰 and Liu Zhiyuan, Sun Maosong},
  year = {2016},
  month = feb,
  journal = {计算机研究与发展},
  volume = {53},
  number = {2},
  pages = {247},
  issn = {1000-1239},
  doi = {10.7544/issn1000-1239.2016.20160020},
  abstract = {人们构建的知识库通常被表示为网络形式，节点代表实体，连边代表...},
  langid = {chinese},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\BMH5YCW3\\刘知远，孙茂松，林衍凯，谢若冰 和 Liu Zhiyuan - 2016 - 知识表示学习研究进展.pdf;C\:\\Users\\Leo\\Zotero\\storage\\67B3A3PT\\issn1000-1239.2016.html}
}

@article{luJiYuBLEUDeShuXueZhuGuanTiZiDongYueJuanDeFangFa2019,
  title = {{基于BLEU的数学主观题自动阅卷的方法}},
  author = {卢, 雨轩 and 孙, 玥莹},
  year = {2019},
  journal = {管理观察},
  number = {03},
  pages = {121--124},
  issn = {1674-2877},
  abstract = {数学主观题自动阅卷的主要思想是计算学生答案和试题标准答案的相似度,类似于机器翻译中通过计算机器生成的译文与参考译文的相似度来评价译文的质量。本文基于两者之间的相通之处,提出了一种基于机器翻译评分指标BLEU的数学主观题自动阅卷方法,在高二年级数学真实考题上进行实验,准确率达到88.11\%。},
  langid = {chinese},
  keywords = {⛔ No DOI found,Automatic marking,BLEU,Mathematical subjective questions,Text similarity,数学主观题,文本相似度,自动阅卷},
  annotation = {00000}
}

@inproceedings{maStateoftheartChineseWord2018,
  title = {State-of-the-Art {{Chinese Word Segmentation}} with {{Bi-LSTMs}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ma, Ji and Ganchev, Kuzman and Weiss, David},
  year = {2018},
  month = oct,
  pages = {4902--4908},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1529},
  abstract = {A wide variety of neural-network architectures have been proposed for the task of Chinese word segmentation. Surprisingly, we find that a bidirectional LSTM model, when combined with standard deep learning techniques and best practices, can achieve better accuracy on many of the popular datasets as compared to models based on more complex neuralnetwork architectures. Furthermore, our error analysis shows that out-of-vocabulary words remain challenging for neural-network models, and many of the remaining errors are unlikely to be fixed through architecture changes. Instead, more effort should be made on exploring resources for further improvement.},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\Q846NUKD\\Ma 等。 - 2018 - State-of-the-art Chinese Word Segmentation with Bi.pdf}
}

@inproceedings{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural...},
  langid = {english},
  annotation = {00710},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\M4FW2FVT\\Micikevicius 等。 - 2018 - Mixed Precision Training.pdf;C\:\\Users\\Leo\\Zotero\\storage\\H5B25TG4\\forum.html}
}

@article{minMetaICLLearningLearn2021,
  title = {{{MetaICL}}: {{Learning}} to {{Learn In Context}}},
  shorttitle = {{{MetaICL}}},
  author = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.15943},
  eprint = {2110.15943},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\TABATDAS\\Min 等。 - 2021 - MetaICL Learning to Learn In Context.pdf;C\:\\Users\\Leo\\Zotero\\storage\\SF8E77WH\\2110.html}
}

@article{ModelingRelationalData,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}} | {{SpringerLink}}},
  doi = {10.1007/978-3-319-93417-4_38},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\AKLF52BS\\Modeling Relational Data with Graph Convolutional .pdf;C\:\\Users\\Leo\\Zotero\\storage\\UIKNY9MU\\Modeling Relational Data with Graph Convolutional .pdf}
}

@article{moussiadesPDetectClusteringApproach2005,
  title = {{{PDetect}}: {{A}} Clustering Approach for Detecting Plagiarism in Source Code Datasets},
  shorttitle = {{{PDetect}}},
  author = {Moussiades, Lefteris and Vakali, Athena},
  year = {2005},
  journal = {The computer journal},
  volume = {48},
  number = {6},
  pages = {651--661},
  publisher = {{Oxford University Press}},
  doi = {10.1093/comjnl/bxh119},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\9FN83Z73\\Moussiades 和 Vakali - 2005 - PDetect A clustering approach for detecting plagi.pdf;C\:\\Users\\Leo\\Zotero\\storage\\L6TMEVI5\\358280.html}
}

@inproceedings{nallapatiAbstractiveTextSummarization2016,
  title = {Abstractive {{Text Summarization}} Using {{Sequence-to-sequence RNNs}} and {{Beyond}}},
  booktitle = {Proceedings of {{The}} 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}}},
  author = {Nallapati, Ramesh and Zhou, Bowen and {dos Santos}, Cicero and G{\.u}l{\c c}ehre, {\c C}a{\u g}lar and Xiang, Bing},
  year = {2016},
  month = aug,
  pages = {280--290},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/K16-1028},
  langid = {english},
  annotation = {01587},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\WYABYYG4\\Nallapati 等。 - 2016 - Abstractive Text Summarization using Sequence-to-s.pdf}
}

@article{narayananEfficientLargeScaleLanguage2021,
  title = {Efficient {{Large-Scale Language Model Training}} on {{GPU Clusters Using Megatron-LM}}},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  year = {2021},
  month = aug,
  journal = {arXiv:2104.04473 [cs]},
  eprint = {2104.04473},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Distributed; Parallel; and Cluster Computing},
  annotation = {00000},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\28GA6ACL\\Narayanan 等。 - 2021 - Efficient Large-Scale Language Model Training on G.pdf;C\:\\Users\\Leo\\Zotero\\storage\\VZGR43EW\\2104.html}
}

@inproceedings{nentidisOverviewBioASQ8a2020,
  title = {Overview of {{BioASQ}} 8a and 8b: {{Results}} of the {{Eighth Edition}} of the {{BioASQ Tasks}} a and b.},
  shorttitle = {Overview of {{BioASQ}} 8a and 8b},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Nentidis, Anastasios and Krithara, Anastasia and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  year = {2020},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\P8NLJJFZ\\Nentidis 等。 - 2020 - Overview of BioASQ 8a and 8b Results of the Eight.pdf}
}

@article{ouyangSQuADBioASQAnalysis,
  title = {{{SQuAD}} to {{BioASQ}}: Analysis of General to Specific},
  author = {Ouyang, Karen},
  pages = {7},
  abstract = {As biomedical information in the form of publications and electronic health records (EHR) increases at an increasingly fast pace, there is clear utility in having systems that can automatically handle information extraction, summarization, and question answering tasks. While there have been significant strides in improving language tasks for general language, addressing domain-specific contexts still remains challenging. In this project, I apply and fine-tune models to the SQuAD dataset and further modify/adapt for biomedical domain-specific question answering. I evaluated and compared performance on the SQuAD dataset and BioASQ, a biomedical literature QA dataset, with the goal of analyzing and developing approaches to leverage unsupervised language models for domain-specific applications. Upon generating various fine-tuned models, the best performance for general language SQuAD QA achieved an F1 score of 76.717, EM score of 73.379, and for biomedical-specific BioASQ QA achieved an F1 score of 70.348 and EM score of 49.902.},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\7A7J5MCT\\Ouyang - SQuAD to BioASQ analysis of general to speciﬁc.pdf}
}

@article{pampariEmrQALargeCorpus2018,
  title = {{{emrQA}}: {{A Large Corpus}} for {{Question Answering}} on {{Electronic Medical Records}}},
  shorttitle = {{{emrQA}}},
  author = {Pampari, Anusri and Raghavan, Preethi and Liang, Jennifer and Peng, Jian},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.00732 [cs]},
  eprint = {1809.00732},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets\textsection. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ questionanswer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\7VUGNHPZ\\Pampari 等。 - 2018 - emrQA A Large Corpus for Question Answering on El.pdf}
}

@inproceedings{pappasAUEBNLPBioASQBiomedical2020,
  title = {{{AUEB-NLP}} at {{BioASQ}} 8: {{Biomedical Document}} and {{Snippet Retrieval}}.},
  shorttitle = {{{AUEB-NLP}} at {{BioASQ}} 8},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion},
  year = {2020},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\5C3LF6YC\\Pappas 等。 - 2020 - AUEB-NLP at BioASQ 8 Biomedical Document and Snip.pdf}
}

@inproceedings{pappasBioMRCDatasetBiomedical2020,
  title = {{{BioMRC}}: {{A Dataset}} for {{Biomedical Machine Reading Comprehension}}},
  shorttitle = {{{BioMRC}}},
  booktitle = {Proceedings of the 19th {{SIGBioMed Workshop}} on {{Biomedical Language Processing}}},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion and McDonald, Ryan},
  year = {2020},
  month = jul,
  pages = {140--149},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.bionlp-1.15},
  abstract = {We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.},
  langid = {english},
  annotation = {00009},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\JUEIH73J\\Pappas 等。 - 2020 - BioMRC A Dataset for Biomedical Machine Reading C.pdf}
}

@inproceedings{pappasBioReadNewDataset2018,
  title = {{{BioRead}}: {{A New Dataset}} for {{Biomedical Reading Comprehension}}},
  shorttitle = {{{BioRead}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Pappas, Dimitris and Androutsopoulos, Ion and Papageorgiou, Haris},
  year = {2018},
  month = may,
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Miyazaki, Japan}},
  langid = {english},
  annotation = {00012},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\4KUPTHRR\\Pappas 等。 - 2018 - BioRead A New Dataset for Biomedical Reading Comp.pdf}
}

@article{parrANTLRPredicatedLLParser1995,
  title = {{{ANTLR}}: A Predicated-{{{\emph{LL}}}}{\emph{(k)}} Parser Generator},
  shorttitle = {{{ANTLR}}},
  author = {Parr, T. J. and Quong, R. W.},
  year = {1995},
  month = jul,
  journal = {Software\textemdash Practice \& Experience},
  volume = {25},
  number = {7},
  pages = {789--810},
  issn = {0038-0644},
  doi = {10.1002/spe.4380250705},
  keywords = {compiler,LL(k) parser,parser generator,parsing,predicates}
}

@inproceedings{pawelczakBenefitsDrawbacksSource2018,
  title = {Benefits and Drawbacks of Source Code Plagiarism Detection in Engineering Education},
  booktitle = {2018 {{IEEE Global Engineering Education Conference}} ({{EDUCON}})},
  author = {Pawelczak, Dieter},
  year = {2018},
  month = apr,
  pages = {1048--1056},
  issn = {2165-9567},
  doi = {10.1109/EDUCON.2018.8363346},
  abstract = {Source code plagiarism is wide spread in beginners' programming courses. Especially, if programming is a minor subject, as for instance in engineering degrees. It is very tempting for students during a programming assignment to use a working copy of a fellow student rather than struggling with the time-consuming coding by themselves. But as learning programming requires a significant personal commitment, we confirm the results of other studies, that cheating leads to higher failure rates and lower scores in the examination. Automatic plagiarism detection systems are therefore measures against cheating. We analyzed the students' achievements and opinions during the last 5 years of operating an automated assessment system with plagiarism detection. The paper discusses in detail the benefits of such a system, e.g. the equal treatment of all students compared to manual plagiarism checks, and shows also the disadvantages, e.g. code obfuscation, that students perform in order to circumvent the system.},
  keywords = {electronic assessment,Encoding,engineering education,Engineering education,plagiarism,Plagiarism,Programming profession,source code plagiarism,Task analysis,teaching programming,Tools},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\XKGHQCJQ\\Pawelczak - 2018 - Benefits and drawbacks of source code plagiarism d.pdf;C\:\\Users\\Leo\\Zotero\\storage\\2MYH5CKF\\8363346.html}
}

@misc{PytorchFenBuShiXunLian,
  title = {{Pytorch 分布式训练}},
  journal = {知乎专栏},
  abstract = {个人整理，其中分布式代码均亲自验证过，可作为模板使用。 第一部分中，部分图片来自知乎提问部分，文中有链接，可以看更详细的讲解，侵删。 未经许可，严禁转载！！！ 内容较多，整理的的有些乱，将就着看吧。 能\ldots},
  howpublished = {https://zhuanlan.zhihu.com/p/76638962},
  langid = {chinese},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\29YWXMJA\\76638962.html}
}

@book{qingmufenglangZiZhiBianYiQi,
  title = {自制编译器},
  author = {青木峰郎},
  isbn = {978-7-115-42218-7},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\F9CILFDV\\26806041.html}
}

@inproceedings{rajpurkarKnowWhatYou2018,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  year = {2018},
  pages = {784--789},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2124},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
  annotation = {01226},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\KU2KMK6R\\Rajpurkar 等。 - 2018 - Know What You Don't Know Unanswerable Questions f.pdf}
}

@inproceedings{rajpurkarSQuAD1000002016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  pages = {2383--2392},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1264},
  annotation = {03991},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\A33Z7RCT\\Rajpurkar 等。 - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf}
}

@inproceedings{richardsonMCTestChallengeDataset2013,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  shorttitle = {{{MCTest}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
  year = {2013},
  month = oct,
  pages = {193--203},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}},
  annotation = {00610},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\7IP9U5JU\\Richardson 等。 - 2013 - MCTest A Challenge Dataset for the Open-Domain Ma.pdf}
}

@inproceedings{seoBidirectionalAttentionFlow2017,
  title = {Bidirectional {{Attention Flow}} for {{Machine Comprehension}}},
  booktitle = {The {{International Conference}} on {{Learning Representations}}},
  author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  year = {2017},
  eprint = {1611.01603},
  eprinttype = {arxiv},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\KZXKDLII\\Seo 等。 - 2018 - Bidirectional Attention Flow for Machine Comprehen.pdf}
}

@article{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year = {2020},
  month = mar,
  journal = {arXiv:1909.08053 [cs]},
  eprint = {1909.08053},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  annotation = {00333},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\XA6PLPUA\\Shoeybi 等。 - 2020 - Megatron-LM Training Multi-Billion Parameter Lang.pdf;C\:\\Users\\Leo\\Zotero\\storage\\9VZ6SH5N\\1909.html}
}

@article{tangRapidlyBootstrappingQuestion2020,
  title = {Rapidly {{Bootstrapping}} a {{Question Answering Dataset}} for {{COVID-19}}},
  author = {Tang, Raphael and Nogueira, Rodrigo and Zhang, Edwin and Gupta, Nikhil and Cam, Phuong and Cho, Kyunghyun and Lin, Jimmy},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.11339 [cs]},
  eprint = {2004.11339},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at http://covidqa.ai/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {00047}
}

@article{trischlerNewsQAMachineComprehension2017,
  title = {{{NewsQA}}: {{A Machine Comprehension Dataset}}},
  shorttitle = {{{NewsQA}}},
  author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  year = {2017},
  month = feb,
  journal = {arXiv:1611.09830 [cs]},
  eprint = {1611.09830},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {00520},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\XB6H3NAK\\Trischler 等。 - 2017 - NewsQA A Machine Comprehension Dataset.pdf;C\:\\Users\\Leo\\Zotero\\storage\\8LLD6IFG\\1611.html}
}

@article{tsatsaronisOverviewBIOASQLargescale2015,
  title = {An Overview of the {{BIOASQ}} Large-Scale Biomedical Semantic Indexing and Question Answering Competition},
  author = {Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R. and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and Almirantis, Yannis and Pavlopoulos, John and Baskiotis, Nicolas and Gallinari, Patrick and Arti{\'e}res, Thierry and Ngomo, Axel-Cyrille Ngonga and Heino, Norman and Gaussier, Eric and {Barrio-Alvers}, Liliana and Schroeder, Michael and Androutsopoulos, Ion and Paliouras, Georgios},
  year = {2015},
  month = apr,
  journal = {BMC Bioinformatics},
  volume = {16},
  number = {1},
  pages = {138},
  issn = {1471-2105},
  doi = {10.1186/s12859-015-0564-6},
  abstract = {This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies.},
  langid = {english},
  keywords = {BioASQ Competition,Hierarchical Text Classification,Information retrieval,Multi-document text summarization,Passage retrieval,Question answering,Semantic indexing},
  annotation = {00354},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\N9KFVPJK\\Tsatsaronis 等。 - 2015 - An overview of the BIOASQ large-scale biomedical s.pdf;C\:\\Users\\Leo\\Zotero\\storage\\87VT3PEA\\s12859-015-0564-6.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  annotation = {34483},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\UNS5V5ZS\\Vaswani 等。 - 2017 - Attention is All you Need.pdf}
}

@article{wangCoKEContextualizedKnowledge2020,
  title = {{{CoKE}}: {{Contextualized Knowledge Graph Embedding}}},
  shorttitle = {{{CoKE}}},
  author = {Wang, Quan and Huang, Pingping and Wang, Haifeng and Dai, Songtai and Jiang, Wenbin and Liu, Jing and Lyu, Yajuan and Zhu, Yong and Wu, Hua},
  year = {2020},
  month = apr,
  journal = {arXiv:1911.02168 [cs]},
  eprint = {1911.02168},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 21.0\% in H@10 on path query answering. Our code is available at \textbackslash url\{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE\}.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\4AYBJ8AT\\Wang 等。 - 2020 - CoKE Contextualized Knowledge Graph Embedding.pdf;C\:\\Users\\Leo\\Zotero\\storage\\RZXHPRNW\\1911.html}
}

@article{wangKnowledgeGraphEmbedding2014,
  title = {Knowledge {{Graph Embedding}} by {{Translating}} on {{Hyperplanes}}},
  author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {TransH},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\C7UJYBT8\\Wang 等。 - 2014 - Knowledge Graph Embedding by Translating on Hyperp.pdf}
}

@article{wangxiaojieJiQiYueDuLiJieDeYanJiuJinZhan2019,
  title = {{机器阅读理解的研究进展}},
  author = {王小捷 and {Xiao-jie}, WANG and 白子薇 and {Zi-wei}, B. A. I. and 李可 and Ke, L. I. and 袁彩霞 and {Cai-xia}, YUAN},
  year = {2019},
  month = dec,
  publisher = {{北京邮电大学学报}},
  doi = {10.13190/j.jbupt.2019-111},
  abstract = {为便于厘清机器阅读理解任务的研究现状，按照答案来源，将机器阅读理解分为完形填空、片段选择、多项选择和答案生成4类.在统一的编码器-交互与推理-输出框架下对此4类任务的已有研究进行了综述，并描述了2种对此框架的可能扩展；最后讨论了机器阅读理解未来需要解决的问题.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {cn},
  annotation = {00000},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\TX28BV4T\\机器阅读理解的研究进展_王小捷.pdf;C\:\\Users\\Leo\\Zotero\\storage\\EYA7WCLQ\\20190601.html}
}

@article{weissenbornMakingNeuralQA2017,
  title = {Making {{Neural QA}} as {{Simple}} as {{Possible}} but Not {{Simpler}}},
  author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.04816 [cs]},
  eprint = {1703.04816},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {00188},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\N863QUC3\\Weissenborn 等。 - 2017 - Making Neural QA as Simple as Possible but not Sim.pdf;C\:\\Users\\Leo\\Zotero\\storage\\GBEQ8PHA\\1703.html}
}

@article{westonAICompleteQuestionAnswering2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  journal = {arXiv:1502.05698 [cs, stat]},
  eprint = {1502.05698},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning},
  annotation = {00961},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\CU656H7H\\Weston 等。 - 2015 - Towards AI-Complete Question Answering A Set of P.pdf;C\:\\Users\\Leo\\Zotero\\storage\\7BUVGBFL\\1502.html}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1609.08144},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {05182},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\THL2UX7K\\Wu 等。 - 2016 - Google's Neural Machine Translation System Bridgi.pdf;C\:\\Users\\Leo\\Zotero\\storage\\D4EGAUTT\\1609.html}
}

@misc{yangBERTMeetsChinese2019,
  title = {{{BERT Meets Chinese Word Segmentation}}},
  author = {Yang, Haiqin},
  year = {2019},
  month = sep,
  number = {arXiv:1909.09292},
  eprint = {1909.09292},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1909.09292},
  abstract = {Chinese word segmentation (CWS) is a fundamental task for Chinese language understanding. Recently, neural network-based models have attained superior performance in solving the in-domain CWS task. Last year, Bidirectional Encoder Representation from Transformers (BERT), a new language representation model, has been proposed as a backbone model for many natural language tasks and redefined the corresponding performance. The excellent performance of BERT motivates us to apply it to solve the CWS task. By conducting intensive experiments in the benchmark datasets from the second International Chinese Word Segmentation Bake-off, we obtain several keen observations. BERT can slightly improve the performance even when the datasets contain the issue of labeling inconsistency. When applying sufficiently learned features, Softmax, a simpler classifier, can attain the same performance as that of a more complicated classifier, e.g., Conditional Random Field (CRF). The performance of BERT usually increases as the model size increases. The features extracted by BERT can be also applied as good candidates for other neural network models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\MQW95ZSF\\Yang - 2019 - BERT Meets Chinese Word Segmentation.pdf;C\:\\Users\\Leo\\Zotero\\storage\\SMB5SRHV\\1909.html}
}

@article{yoonSequenceTaggingBiomedical2021,
  title = {Sequence {{Tagging}} for {{Biomedical Extractive Question Answering}}},
  author = {Yoon, Wonjin and Jackson, Richard and Kang, Jaewoo and Lagerberg, Aron},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.07535},
  eprint = {2104.07535},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\A4NIG3E2\\Yoon 等。 - 2021 - Sequence Tagging for Biomedical Extractive Questio.pdf;C\:\\Users\\Leo\\Zotero\\storage\\7ZKEYYMB\\2104.html}
}

@article{zhangBioWordVecImprovingBiomedical2019,
  title = {{{BioWordVec}}, Improving Biomedical Word Embeddings with Subword Information and {{MeSH}}},
  author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {52},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0055-0},
  langid = {english},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\HK799MK5\\Zhang 等。 - 2019 - BioWordVec, improving biomedical word embeddings w.pdf}
}

@article{zhengDGLKETrainingKnowledge2020,
  title = {{{DGL-KE}}: {{Training Knowledge Graph Embeddings}} at {{Scale}}},
  shorttitle = {{{DGL-KE}}},
  author = {Zheng, Da and Song, Xiang and Ma, Chao and Tan, Zeyuan and Ye, Zihao and Dong, Jin and Xiong, Hao and Zhang, Zheng and Karypis, George},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08532 [cs]},
  eprint = {2004.08532},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge graphs have emerged as a key abstraction for organizing information in diverse domains and their embeddings are increasingly used to harness their information in various information retrieval and machine learning tasks. However, the ever growing size of knowledge graphs requires computationally efficient algorithms capable of scaling to graphs with millions of nodes and billions of edges. This paper presents DGL-KE, an open-source package to efficiently compute knowledge graph embeddings. DGL-KE introduces various novel optimizations that accelerate training on knowledge graphs with millions of nodes and billions of edges using multi-processing, multi-GPU, and distributed parallelism. These optimizations are designed to increase data locality, reduce communication overhead, overlap computations with memory accesses, and achieve high operation efficiency. Experiments on knowledge graphs consisting of over 86M nodes and 338M edges show that DGL-KE can compute embeddings in 100 minutes on a EC2 instance with 8 GPUs and 30 minutes on an EC2 cluster with 4 machines with 48 cores/machine. These results represent a 2\texttimes{} {$\sim$} 5\texttimes{} speedup over the best competing approaches. DGL-KE is available on https://github.com/awslabs/dgl-ke.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {C\:\\Users\\Leo\\Zotero\\storage\\DKYQ6GS5\\Zheng 等。 - 2020 - DGL-KE Training Knowledge Graph Embeddings at Sca.pdf}
}


